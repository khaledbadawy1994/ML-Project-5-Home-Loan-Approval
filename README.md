# ML-Project-5-Home-Loan-Approval

#Loading Packages

importÂ pandasÂ asÂ pdimportÂ numpyÂ asÂ npÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â #Â ForÂ mathematicalÂ calculationsimportÂ seabornÂ asÂ snsÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â #Â ForÂ dataÂ visualizationimportÂ matplotlib.pyplotÂ asÂ pltÂ Â Â Â Â Â Â Â #Â ForÂ plottingÂ graphs%matplotlibÂ inlineimportÂ warningsÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â #Â ToÂ ignoreÂ anyÂ warningswarnings.filterwarnings("ignore")importÂ plotly.expressÂ asÂ pximportÂ plotly.graph_objectsÂ asÂ gofromÂ plotly.subplotsÂ importÂ make_subplots

fromÂ google.colabÂ importÂ drivedrive.mount('/content/drive')

Mounted at /content/drive
train=pd.read_csv("/content/drive/MyDrive/loan_sanction_train.csv")test=pd.read_csv("/content/drive/MyDrive/loan_sanction_test.csv")

train.columns

test.columns

#Â PrintÂ dataÂ typesÂ forÂ eachÂ variable
train.dtypes

train.shape,Â test.shape

train['Loan_Status'].value_counts()

#Â NormalizeÂ canÂ beÂ setÂ toÂ TrueÂ toÂ printÂ proportionsÂ insteadÂ ofÂ number
train['Loan_Status'].value_counts(normalize=True)

plt.figure(figsize=(8,Â 6))Â Â #Â AdjustÂ theÂ figureÂ sizeÂ asÂ needed

#Â PlottingÂ theÂ barÂ chart
train['Loan_Status'].value_counts().plot.bar(color=['green',Â 'red'])Â Â #Â CustomizeÂ colorsÂ ifÂ desired

#Â AddingÂ labelsÂ andÂ title
plt.title('LoanÂ ApprovalÂ Status')
plt.xlabel('LoanÂ Status')
plt.ylabel('Count')

#Â DisplayÂ theÂ plot
plt.show()

422(around 69%) people out of 614 got the approval.

pd.crosstab(train['Credit_History'],Â train['Loan_Status']).plot.bar(colorÂ =Â ['orangered',Â 'palegreen'],Â figsizeÂ =Â [15,4])
plt.xticks(rotationÂ =Â 45)
plt.title('LoanÂ StatusÂ /Â CreditÂ History')
plt.grid()
plt.ylabel('NumberÂ ofÂ loans');

pd.crosstab(train['Credit_History'],Â train['Loan_Status'])

train.head()

train.info

#Â CheckingÂ StatisticalÂ data
train.describe()

Note:

#Minimum Salary of Applicant is 150 Maximum Salary of Applicant is 81000 Average Salary of Applicant is 5403 Mean loan amt is 146 Mean loan term is 342

train.isnull()

#Â ChekingÂ ifÂ thereÂ isÂ nullÂ value

train.isnull().sum()

train_cleanedÂ =train.dropna()
train_cleaned

#Â DataÂ Cleaning

train_cleaned.isnull().sum()

#Â DropÂ rowsÂ withÂ anyÂ missingÂ values
train_drop_rowÂ =Â train.dropna()
train_drop_row

#Â DropÂ columnsÂ withÂ anyÂ missingÂ values
train_drop_columnsÂ =Â train.dropna(axis=1)
train_drop_columns

#Â DropÂ rowsÂ onlyÂ ifÂ allÂ columnsÂ haveÂ missingÂ values
train_drop_row_allÂ =Â train.dropna(how="all")
train_drop_row_all

#train

train.duplicated().any()

#There are no duplicated rows

#Â RemoveÂ NaNÂ valuesÂ inÂ aÂ specificÂ column
trainÂ =Â train.dropna(subset=['LoanAmount'])
train

has_nanÂ =Â train.isna().any().any().sum()

train_without_nan_rowsÂ =Â train.dropna(axis=0)
train

#Missing Value and Outlier Treatment

train.isnull().sum()

#For numerical variables: imputation using mean or median For categorical variables: imputation using mode

train['Gender'].fillna(train['Gender'].mode()[0],Â inplace=True)
train['Married'].fillna(train['Married'].mode()[0],Â inplace=True)
train['Dependents'].fillna(train['Dependents'].mode()[0],Â inplace=True)
train['Self_Employed'].fillna(train['Self_Employed'].mode()[0],Â inplace=True)
train['Credit_History'].fillna(train['Credit_History'].mode()[0],Â inplace=True)
train['Loan_Amount_Term'].value_counts()

#It can be seen that in the loan amount term variable, the value of 360 is repeated the most. So we will replace the missing values in this variable using the mode of this variable.

train['Loan_Amount_Term'].fillna(train['Loan_Amount_Term'].mode()[0],Â inplace=True)
train['LoanAmount'].fillna(train['LoanAmount'].median(),Â inplace=True)
train.isnull().sum()

test['Gender'].fillna(train['Gender'].mode()[0],Â inplace=True)
test['Dependents'].fillna(train['Dependents'].mode()[0],Â inplace=True)
test['Self_Employed'].fillna(train['Self_Employed'].mode()[0],Â inplace=True)
test['Credit_History'].fillna(train['Credit_History'].mode()[0],Â inplace=True)
test['Loan_Amount_Term'].fillna(train['Loan_Amount_Term'].mode()[0],Â inplace=True)
test['LoanAmount'].fillna(train['LoanAmount'].median(),Â inplace=True)

#Outlier Treatment

train['LoanAmount_log']Â =Â np.log(train['LoanAmount'])
train['LoanAmount_log'].hist(bins=20)
test['LoanAmount_log']Â =Â np.log(test['LoanAmount'])

#Now the distribution looks much closer to normal and the effect of extreme values has been significantly subsided.

plt.figure(figsize=Â [15,5])
sns.boxplot(dataÂ =train)
plt.style.use('ggplot')
plt.grid()

defÂ detect_outlier(col):
Â Â Â Â '''
Â Â Â Â functionÂ toÂ detectÂ outliersÂ (usingÂ zÂ score)
Â Â Â Â '''
Â Â Â Â outliersÂ =Â []
Â Â Â Â thresholdÂ =Â 3Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â #Â seuilÂ standard
Â Â Â Â meanÂ =Â np.mean(col)
Â Â Â Â stdÂ =Â np.std(col)

Â Â Â Â forÂ iÂ inÂ colÂ :
Â Â Â Â Â Â Â Â z_scoreÂ =Â (iÂ -Â mean)Â /Â stdÂ Â Â Â Â Â Â Â Â Â Â Â #Â formuleÂ z-score
Â Â Â Â Â Â Â Â ifÂ np.abs(z_score)Â >Â thresholdÂ :
Â Â Â Â Â Â Â Â Â Â Â Â outliers.append(i)
Â Â Â Â returnÂ outliers
    
detect_outlier(train['ApplicantIncome'])Â Â Â #Â dropÂ 81000,Â 63000

detect_outlier(train['CoapplicantIncome'])Â Â Â Â #Â dropÂ 41000

train.isnull().sum()

#dependentsÂ column
train['Dependents']Â =Â train['Dependents'].fillna('0')
train.isna().sum()

#MarriedÂ column
trainÂ =Â train[train['Married'].notna()]
train.isna().sum()

#Self_employedÂ column
train['Self_Employed']Â =Â train['Self_Employed'].fillna('No')
train.isna().sum()

#LoanÂ amountÂ column
train['LoanAmount']Â =Â train['LoanAmount'].fillna(train['LoanAmount'].median())
train.isna().sum()

#LoanÂ amountÂ termÂ column
train['Loan_Amount_Term']Â =Â train['Loan_Amount_Term'].fillna(train['Loan_Amount_Term'].min())
train.isna().sum()

#creditÂ historyÂ column

train['Credit_History']Â =Â train['Credit_History'].fillna(0)

train.isna().sum()

train.shape

#Now that all the null values are succesfully removed, lets have a look at the columns individually.

train['Gender'].unique()

train['Married'].unique()

train['Dependents'].unique()

train['Education'].unique()

train['Self_Employed'].unique()

train['Credit_History'].unique()

train['Property_Area'].unique()

train['Loan_Status'].unique()

train['Dependents'].unique()

#Credit History column values can be changed to int.

train['Credit_History']Â =Â train['Credit_History'].astype(int)
train['Credit_History'].dtype

train.head()

#Data Visualization

#Â CreateÂ aÂ DataFrameÂ withÂ theÂ countsÂ andÂ percentages
gender_countsÂ =Â train['Gender'].value_counts(normalize=True).reset_index()
gender_counts.columnsÂ =Â ['Gender',Â 'Percentage']

#Â CreateÂ anÂ advancedÂ barÂ chartÂ usingÂ PlotlyÂ Express
figÂ =Â px.bar(gender_counts,
Â Â Â Â Â Â Â Â Â Â Â Â Â x='Gender',
Â Â Â Â Â Â Â Â Â Â Â Â Â y='Percentage',
Â Â Â Â Â Â Â Â Â Â Â Â Â text='Percentage',
Â Â Â Â Â Â Â Â Â Â Â Â Â labels={'Percentage':Â 'Percentage'},
Â Â Â Â Â Â Â Â Â Â Â Â Â color='Gender',
Â Â Â Â Â Â Â Â Â Â Â Â Â color_discrete_sequence=['blue',Â 'orange'],Â Â #Â CustomizeÂ colorsÂ ifÂ desired
Â Â Â Â Â Â Â Â Â Â Â Â Â title='GenderÂ Distribution',
Â Â Â Â Â Â Â Â Â Â Â Â Â templateÂ =Â 'plotly_dark',
Â Â Â Â Â Â Â Â Â Â Â Â Â hover_data={'Gender':Â False,Â 'Percentage':Â ':.2%'},
Â Â Â Â Â Â Â Â Â Â Â Â Â width=800,Â height=500)

#Â CustomizeÂ layout
fig.update_layout(
Â Â Â Â xaxis_title='Gender',
Â Â Â Â yaxis_title='Percentage',
Â Â Â Â showlegend=False,Â Â #Â HideÂ legendÂ forÂ thisÂ specificÂ chart
Â Â Â Â bargap=0.1,Â Â #Â SetÂ gapÂ betweenÂ bars
)

#Â DisplayÂ theÂ plot
fig.show()

#Â CreateÂ aÂ DataFrameÂ withÂ theÂ countsÂ andÂ percentages
married_countsÂ =Â train['Married'].value_counts(normalize=True).reset_index()
married_counts.columnsÂ =Â ['Married',Â 'Percentage']

#Â CreateÂ anÂ advancedÂ barÂ chartÂ usingÂ PlotlyÂ Express
figÂ =Â px.bar(married_counts,
Â Â Â Â Â Â Â Â Â Â Â Â Â x='Married',
Â Â Â Â Â Â Â Â Â Â Â Â Â y='Percentage',
Â Â Â Â Â Â Â Â Â Â Â Â Â text='Percentage',
Â Â Â Â Â Â Â Â Â Â Â Â Â labels={'Percentage':Â 'Percentage'},
Â Â Â Â Â Â Â Â Â Â Â Â Â color='Married',
Â Â Â Â Â Â Â Â Â Â Â Â Â color_discrete_sequence=['#636EFA',Â '#EF553B'],Â Â #Â CustomizeÂ colorsÂ ifÂ desired
Â Â Â Â Â Â Â Â Â Â Â Â Â title='MaritalÂ StatusÂ Distribution',
Â Â Â Â Â Â Â Â Â Â Â Â Â hover_data={'Married':Â False,Â 'Percentage':Â ':.2%'},
Â Â Â Â Â Â Â Â Â Â Â Â Â width=800,Â height=500)

#Â CustomizeÂ layout
fig.update_layout(
Â Â Â Â xaxis_title='MaritalÂ Status',
Â Â Â Â yaxis_title='Percentage',
Â Â Â Â showlegend=False,Â Â #Â HideÂ legendÂ forÂ thisÂ specificÂ chart
Â Â Â Â bargap=0.1,Â Â #Â SetÂ gapÂ betweenÂ bars
Â Â Â Â template='plotly_dark',Â Â #Â SetÂ darkÂ template
)

#Â DisplayÂ theÂ plot
fig.show()

#Â CreateÂ aÂ DataFrameÂ withÂ theÂ countsÂ andÂ percentages
self_employed_countsÂ =Â train['Self_Employed'].value_counts(normalize=True).reset_index()
self_employed_counts.columnsÂ =Â ['Self_Employed',Â 'Percentage']

#Â CreateÂ anÂ advancedÂ barÂ chartÂ usingÂ PlotlyÂ Express
figÂ =Â px.bar(self_employed_counts,
Â Â Â Â Â Â Â Â Â Â Â Â Â x='Self_Employed',
Â Â Â Â Â Â Â Â Â Â Â Â Â y='Percentage',
Â Â Â Â Â Â Â Â Â Â Â Â Â text='Percentage',
Â Â Â Â Â Â Â Â Â Â Â Â Â labels={'Percentage':Â 'Percentage'},
Â Â Â Â Â Â Â Â Â Â Â Â Â color='Self_Employed',
Â Â Â Â Â Â Â Â Â Â Â Â Â color_discrete_sequence=['#1f77b4',Â '#ff7f0e'],Â Â #Â CustomizeÂ colorsÂ ifÂ desired
Â Â Â Â Â Â Â Â Â Â Â Â Â title='SelfÂ EmploymentÂ StatusÂ Distribution',
Â Â Â Â Â Â Â Â Â Â Â Â Â hover_data={'Self_Employed':Â False,Â 'Percentage':Â ':.2%'},
Â Â Â Â Â Â Â Â Â Â Â Â Â width=800,Â height=500)

#Â SetÂ darkÂ template
fig.update_layout(template='plotly_dark')

#Â CustomizeÂ layout
fig.update_layout(
Â Â Â Â xaxis_title='SelfÂ Employed',
Â Â Â Â yaxis_title='Percentage',
Â Â Â Â showlegend=False,Â Â #Â HideÂ legendÂ forÂ thisÂ specificÂ chart
Â Â Â Â bargap=0.1,Â Â #Â SetÂ gapÂ betweenÂ bars
)

#Â DisplayÂ theÂ plot
fig.show()

#Â CreateÂ aÂ DataFrameÂ withÂ theÂ countsÂ andÂ percentages
credit_history_countsÂ =Â train['Credit_History'].value_counts(normalize=True).reset_index()
credit_history_counts.columnsÂ =Â ['Credit_History',Â 'Percentage']

#Â CreateÂ anÂ advancedÂ barÂ chartÂ usingÂ PlotlyÂ Express
figÂ =Â px.bar(credit_history_counts,
Â Â Â Â Â Â Â Â Â Â Â Â Â x='Credit_History',
Â Â Â Â Â Â Â Â Â Â Â Â Â y='Percentage',
Â Â Â Â Â Â Â Â Â Â Â Â Â text='Percentage',
Â Â Â Â Â Â Â Â Â Â Â Â Â labels={'Percentage':Â 'Percentage'},
Â Â Â Â Â Â Â Â Â Â Â Â Â color='Credit_History',
Â Â Â Â Â Â Â Â Â Â Â Â Â color_discrete_sequence=['red',Â 'green'],Â Â #Â CustomizeÂ colorsÂ ifÂ desired
Â Â Â Â Â Â Â Â Â Â Â Â Â title='CreditÂ HistoryÂ Distribution',
Â Â Â Â Â Â Â Â Â Â Â Â Â hover_data={'Credit_History':Â False,Â 'Percentage':Â ':.2%'},
Â Â Â Â Â Â Â Â Â Â Â Â Â width=800,Â height=500)

#Â UseÂ aÂ darkÂ template
fig.update_layout(template='plotly_dark')

#Â CustomizeÂ layout
fig.update_layout(
Â Â Â Â xaxis_title='CreditÂ History',
Â Â Â Â yaxis_title='Percentage',
Â Â Â Â showlegend=False,Â Â #Â HideÂ legendÂ forÂ thisÂ specificÂ chart
Â Â Â Â bargap=0.1,Â Â #Â SetÂ gapÂ betweenÂ bars
Â Â Â Â )

#Â DisplayÂ theÂ plot
fig.show()

ğŸ•µï¸Observations:

80% of applicants in the dataset are male. Around 65% of the applicants in the dataset are married. About 15% of applicants in the dataset are self-employed. About 84% of applicants have repaid their debts.

importÂ plotly.subplotsÂ asÂ sp
importÂ plotly.graph_objectsÂ asÂ go

#Â AssumingÂ 'train'Â isÂ yourÂ DataFrame

#Â CreateÂ subplots
figÂ =Â sp.make_subplots(rows=1,Â cols=3,Â subplot_titles=['Dependents',Â 'Education',Â 'PropertyÂ Area'])

#Â PlotÂ DependentsÂ distribution
fig.add_trace(go.Bar(x=train['Dependents'].value_counts(normalize=True).index,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â y=train['Dependents'].value_counts(normalize=True),
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â name='Dependents'),
Â Â Â Â Â Â Â Â Â Â Â Â Â Â row=1,Â col=1)

#Â PlotÂ EducationÂ distribution
fig.add_trace(go.Bar(x=train['Education'].value_counts(normalize=True).index,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â y=train['Education'].value_counts(normalize=True),
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â name='Education'),
Â Â Â Â Â Â Â Â Â Â Â Â Â Â row=1,Â col=2)

#Â PlotÂ PropertyÂ AreaÂ distribution
fig.add_trace(go.Bar(x=train['Property_Area'].value_counts(normalize=True).index,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â y=train['Property_Area'].value_counts(normalize=True),
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â name='PropertyÂ Area'),
Â Â Â Â Â Â Â Â Â Â Â Â Â Â row=1,Â col=3)

#Â UpdateÂ layout
fig.update_layout(height=500,Â showlegend=False,Â title_text="CategoricalÂ Distributions")

#Â DisplayÂ theÂ plot
fig.show()

ğŸ•µï¸Observations:

Most of the applicants donâ€™t have dependents. About 78% of the applicants are graduates. Most of the applicants are from semi-urban areas.

cat_featuresÂ =Â train.select_dtypes('object').columns
fig,axÂ =Â plt.subplots(2,4,figsize=(20,12))

forÂ i,featureÂ inÂ enumerate(cat_features):
Â Â Â Â orderÂ =Â (
Â Â Â Â Â Â Â Â train[feature]
Â Â Â Â Â Â Â Â .value_counts(normalizeÂ =Â TrueÂ )
Â Â Â Â Â Â Â Â .sort_values(ascending=False).index
Â Â Â Â )
Â Â Â Â sns.countplot(data=train,x=feature,ax=ax[i//4,i%4],orderÂ =Â order)
Â Â Â Â ax[i//4,i%4].set_title(f'{feature}Â Distribution')

#General notes:

most of the applicants are males. most of the applicants are married. most of the applicants have no dependents. most of the applicants are Graduates. most of the applicants aren't self employed. most of the loans are above 360. most of the applicants do have a credit history.

#####Â ConvertÂ Loan_StatusÂ intoÂ LabelÂ EncodingÂ #####
train.loc[:,Â 'Loan_Status']Â =Â train.loc[:,Â 'Loan_Status'].map({'Y':Â 1,Â 'N':Â 0})

train.head()

train.columns

train.loc[:,Â 'Loan_Status'].value_counts()

forÂ colÂ inÂ train.select_dtypes('float'):
Â Â Â Â ifÂ colÂ !=Â 'Credit_History'Â andÂ colÂ !=Â 'Loan_Amount_Term'Â :Â Â Â #Â credit_history(1/0).
Â Â Â Â Â Â Â Â plt.figure(figsize=[10,4])
Â Â Â Â Â Â Â Â plt.title(col)
Â Â Â Â Â Â Â Â sns.distplot(xÂ =Â train[col])

num_featuresÂ =Â train.select_dtypes('float').columns

fig,axÂ =Â plt.subplots(1,3,figsize=(20,5))

forÂ i,featureÂ inÂ enumerate(num_features):
Â Â Â Â sns.boxplot(data=train,x="Loan_Status",y=feature,ax=ax[i])
Â Â Â Â ax[i].set_title(f'{feature}Â BoxÂ Plot')

#Handling outliers

train[train['CoapplicantIncome']<10000].shape
(586, 14)
sns.histplot(train[train['CoapplicantIncome']<10000]['CoapplicantIncome'],Â kde=True);

#we handle outliers by applying log transformation on LoanAmount and ApplicantIncome and handle them in CoapplicantIncome by setting a threshold.

dfÂ =Â train[train['CoapplicantIncome']<10000]

forÂ colÂ inÂ ['LoanAmount','ApplicantIncome']:
Â Â Â Â df[col]Â =Â df[col].apply(np.log1p)
fig,axÂ =Â plt.subplots(1,3,figsize=(20,5))

forÂ i,featureÂ inÂ enumerate(num_features):
Â Â Â Â sns.histplot(data=df,x=feature,ax=ax[i],kde=True,color='Green')
Â Â Â Â ax[i].set_title(f'{feature}Â Distribution')

#Â CreateÂ subplots
figÂ =Â sp.make_subplots(rows=1,Â cols=2,Â subplot_titles=['ApplicantÂ IncomeÂ Distribution',Â 'ApplicantÂ IncomeÂ BoxÂ Plot'])

#Â AddÂ histogramÂ trace
histogram_traceÂ =Â go.Histogram(x=train['ApplicantIncome'],Â nbinsx=30,Â marker=dict(color='rgba(0,Â 123,Â 255,Â 0.7)'))
fig.add_trace(histogram_trace,Â row=1,Â col=1)

#Â AddÂ boxÂ plotÂ trace
box_traceÂ =Â go.Box(x=train['ApplicantIncome'],Â marker=dict(color='rgba(0,Â 123,Â 255,Â 0.7)'))
fig.add_trace(box_trace,Â row=1,Â col=2)

#Â UpdateÂ layout
fig.update_layout(height=500,Â showlegend=False,Â title_text="ApplicantÂ IncomeÂ DistributionÂ andÂ BoxÂ Plot")

#Â DisplayÂ theÂ plot
fig.show()

It can be deduced that the applicant income distribution is not normally distributed because the majority of the data point to the left. In later sections will attempt to normalise the data because normally distributed data is preferred by algorithms.

#Â CreateÂ subplots
figÂ =Â sp.make_subplots(rows=1,Â cols=2,Â subplot_titles=['LoanÂ AmountÂ Distribution',Â 'LoanÂ AmountÂ BoxÂ Plot'])

#Â AddÂ histogramÂ trace
histogram_traceÂ =Â go.Histogram(x=train['LoanAmount'],Â nbinsx=30,Â marker=dict(color='rgba(0,Â 123,Â 255,Â 0.7)'))
fig.add_trace(histogram_trace,Â row=1,Â col=1)

#Â AddÂ boxÂ plotÂ trace
box_traceÂ =Â go.Box(x=train['LoanAmount'],Â marker=dict(color='rgba(0,Â 123,Â 255,Â 0.7)'))
fig.add_trace(box_trace,Â row=1,Â col=2)

#Â UpdateÂ layout
fig.update_layout(height=500,Â showlegend=False,Â title_text="LoanÂ AmountÂ DistributionÂ andÂ BoxÂ Plot")

#Â DisplayÂ theÂ plot
fig.show()

#We see a lot of outliers in this variable and the distribution is fairly normal. We will treat the outliers in later sections.

#Â GroupÂ byÂ 'Loan_Status'Â andÂ calculateÂ meanÂ 'ApplicantIncome'
income_meanÂ =Â train.groupby('Loan_Status')['ApplicantIncome'].mean().reset_index()

#Â CreateÂ advancedÂ barÂ chartÂ usingÂ PlotlyÂ Express
figÂ =Â px.bar(income_mean,Â x='Loan_Status',Â y='ApplicantIncome',
Â Â Â Â Â Â Â Â Â Â Â Â Â color='Loan_Status',
Â Â Â Â Â Â Â Â Â Â Â Â Â labels={'ApplicantIncome':Â 'MeanÂ ApplicantÂ Income'},
Â Â Â Â Â Â Â Â Â Â Â Â Â title='MeanÂ ApplicantÂ IncomeÂ byÂ LoanÂ Status',
Â Â Â Â Â Â Â Â Â Â Â Â Â text='ApplicantIncome',
Â Â Â Â Â Â Â Â Â Â Â Â Â height=500)

#Â UpdateÂ layout
fig.update_layout(xaxis_title='LoanÂ Status',Â yaxis_title='MeanÂ ApplicantÂ Income',
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â showlegend=False,Â barmode='group')

#Â ShowÂ theÂ plot
fig.show()

#It can be inferred that Applicantâ€™s income does not affect the chances of loan approval which contradicts our hypothesis in which we assumed that if the applicantâ€™s income is high the chances of loan approval will also be high.

train['Total_Income']Â =Â train['ApplicantIncome']Â +Â train['CoapplicantIncome']
binsÂ =Â [0,Â 2500,Â 4000,Â 6000,Â 81000]
groupÂ =Â ['Low',Â 'Average',Â 'High',Â 'VeryÂ high']
train['Total_Income_bin']Â =Â pd.cut(train['Total_Income'],Â bins,Â labels=group)
Total_Income_binÂ =Â pd.crosstab(train['Total_Income_bin'],Â train['Loan_Status'])
Total_Income_bin.div(Total_Income_bin.sum(1).astype(float),Â axis=0).plot(kind="bar",Â stacked=True)
plt.xlabel('Total_Income')
plt.ylabel('Percentage')
plt.show()

#We can see that Proportion of loans getting approved for applicants having low Total_Income is very less as compared to that of applicants with Average, High, and Very High Income.

bins=[0,100,200,700]
group=['Low','Average','High']
train['LoanAmount_bin']=pd.cut(train['LoanAmount'],bins,labels=group)
LoanAmount_bin=pd.crosstab(train['LoanAmount_bin'],train['Loan_Status'])
LoanAmount_bin.div(LoanAmount_bin.sum(1).astype(float),Â axis=0).plot(kind="bar",Â stacked=True)
plt.xlabel('LoanAmount')
PÂ =Â plt.ylabel('Percentage')

#It can be seen that the proportion of approved loans is higher for Low and Average Loan Amounts as compared to that of High Loan Amounts which supports our hypothesis which considered that the chances of loan approval will be high when the loan amount is less.

#Â PlottingÂ theÂ histogram
sns.histplot(data=train,Â x="ApplicantIncome",Â kde=True,Â edgecolor="black",Â linewidth=1.2)

#Â CustomizingÂ theÂ plot
plt.title("DistributionÂ ofÂ ApplicantÂ Income",Â weight="bold")
plt.xlabel("ApplicantÂ Income")
plt.ylabel("Frequency")

#Â DisplayÂ theÂ plot
plt.show()

#Based on the provided histogram plot, it is evident that there are outliers in the data, as there are a few data points that deviate significantly from the majority of the values. Most of the income values range from 150 to 10000.

train["ApplicantIncome"].agg(["min",Â "max"])

train["CoapplicantIncome"].agg(["min",Â "max"])

train["Loan_Amount_Term"].agg(['min','max'])

plt.hist(x=train["CoapplicantIncome"],Â edgecolor="black",Â linewidth=1.2)

plt.title("DistributionÂ ofÂ CoapplicantÂ Income",Â weight="bold")
plt.xlabel("CoapplicantÂ Income")
plt.ylabel("Frequency")

plt.show()

#Coapplicant income falls from 0 to 41000, but most of the income falls till 10000

train["LoanAmount"].agg(['min','max'])

#Â PlottingÂ theÂ histogram
sns.histplot(data=train,Â x="LoanAmount",Â kde=True,Â edgecolor="black",Â linewidth=1.2)

plt.title("DistributionÂ ofÂ LoanÂ Amount",Â weight="bold")
plt.xlabel("LoanÂ Amount")
plt.ylabel("Frequency")

plt.show()

#Most of the Loan amt falls under 9 to 200

#Â CreateÂ theÂ histogramÂ plot
sns.histplot(data=train,Â x="Loan_Amount_Term",Â kde=True,Â edgecolor="black",Â linewidth=1.2)

#Â CustomizationÂ options
plt.title("LoanÂ AmountÂ TermÂ Distribution",Â weight="bold")Â Â #Â SetÂ theÂ plotÂ title
plt.xlabel("LoanÂ AmountÂ Term")Â Â #Â SetÂ theÂ x-axisÂ label
plt.ylabel("Frequency")Â Â #Â SetÂ theÂ y-axisÂ label

#Â ShowÂ theÂ plot
plt.show()

#Most of the Loan term falls under 400

train['Credit_History']Â =Â train['Credit_History'].map({1:Â "Yes",Â 0:Â "No"})

##Â CreateÂ countplotÂ ==>Â Credit_History
sns.countplot(data=train,Â x="Credit_History")

##Â labelsÂ andÂ title
plt.xlabel("CreditÂ History")
plt.ylabel("Count")
plt.title("DistributionÂ ofÂ CreditÂ History",Â weight="bold")

##Â ShowÂ theÂ plot
plt.show()

#Most of the applicants credit score is Yes (Good)

married_countÂ =Â train["Married"].value_counts()
married_count

#train

#Most of the applicants credit score is Yes (Good)

gender_countsÂ =Â train["Gender"].value_counts()
gender_counts

#Â AssumingÂ trainÂ isÂ yourÂ DataFrame
Gender_countÂ =Â train["Gender"].value_counts()

#Â CreateÂ aÂ histogram
sns.histplot(data=train,Â x="Gender",Â kde=True,Â edgecolor="black",Â linewidth=1.2)

#Â LabelsÂ andÂ title
plt.xlabel("Gender")
plt.ylabel("Count")
plt.title("DistributionÂ ofÂ Gender",Â weight="bold")

#Â ShowÂ theÂ plot
plt.show()

#Male Applicants are more than Female Applicants

married_countÂ =Â train["Married"].value_counts()
married_count

##Â CreateÂ aÂ pieÂ chart
plt.pie(x=married_count.values,Â labels=married_count.index,Â autopct="%0.2f%%")

##Â title
plt.title("MaritalÂ StatusÂ Distribution")

##Â CustomizeÂ theÂ styling
plt.axis("equal")Â Â #Â SetÂ theÂ aspectÂ ratioÂ toÂ makeÂ theÂ pieÂ circular

##Â AddÂ aÂ legend
plt.legend(loc='best')

##Â ShowÂ theÂ plot
plt.show()

#Most of the Applicants are married

train["Dependents"].unique()

train["Dependents"]Â =Â train["Dependents"].map(
Â Â Â Â {"Zero":"0","One":"1","Two":"2","ThreeÂ plus":"3"}
Â Â Â Â Â )
     
#train

train["Dependents"].value_counts(normalize=True)
Series([], Name: proportion, dtype: float64)
train["Education"].value_counts(normalize=True).to_frame()

train["Gender"].value_counts(normalize=True).to_frame()

train["Married"].value_counts(normalize=True).to_frame()

train["Self_Employed"].value_counts(normalize=True).to_frame()

#Only 14 % of the applicants are selfemployed

##Â CalculateÂ theÂ valueÂ countsÂ ofÂ "Property_Area"
property_countÂ =Â train["Property_Area"].value_counts()
print(property_count)
print("*"*50)

##Â CreateÂ aÂ barÂ plotÂ usingÂ Plotly
figÂ =Â px.bar(x=property_count.index,Â y=property_count.values,Â labels={"x":Â "PropertyÂ Area",Â "y":Â "Count"},
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â title="<b>DistributionÂ ofÂ PropertyÂ Area",Â color_discrete_sequence=px.colors.qualitative.Bold_r)

##Â CustomizeÂ theÂ plot
fig.update_layout(
Â Â Â Â paper_bgcolor="rgb(233,233,233)",
Â Â Â Â plot_bgcolor="white",
)

##Â ShowÂ theÂ plot
fig.show()

#Most of the applicants from semiurban

loan_countÂ =Â train["Loan_Status"].value_counts()
print(loan_count)
print("*"*50)

##Â CreateÂ aÂ pieÂ chart
plt.pie(x=loan_count.values,Â labels=loan_count.index,Â autopct="%0.2f%%",Â shadow=True,Â explode=(0,Â 0.09))

##Â title
plt.title("LoanÂ StatusÂ Distribution",Â weight="bold")

##Â CustomizeÂ theÂ styling
plt.axis("equal")Â Â #Â SetÂ theÂ aspectÂ ratioÂ toÂ makeÂ theÂ pieÂ circular

##Â AddÂ aÂ legend
plt.legend(loc='best')

##Â ShowÂ theÂ plot
plt.show()

#The target feature in this dataset indicates the approval status of loans, distinguishing between acceptance and non-acceptance. It is noteworthy that there is an imbalance in the classes of this target variable, with a substantial disparity in the number of instances between accepted and non-accepted loan statuses.

#Is there a relationship between the applicant's income and loan amount requested?

##Â ApplicantIncomeÂ andÂ LoanAmount
correlation_valueÂ =train["ApplicantIncome"].corr(train["LoanAmount"])
print(f'TheÂ correlationÂ betweenÂ ApplicantÂ IncomeÂ andÂ LoanÂ AmountÂ isÂ {correlation_value:.2f}')

#There is a moderate relationship between income and loan amount.

##Â CreateÂ aÂ scatterÂ plot
sns.scatterplot(data=train,Â x="ApplicantIncome",Â y="LoanAmount")

##Â labelsÂ andÂ title
plt.xlabel("ApplicantÂ Income")
plt.ylabel("LoanÂ Amount")
plt.title("RelationshipÂ betweenÂ ApplicantÂ IncomeÂ andÂ LoanÂ Amount",Â weight="bold")

##Â styleÂ settings
plt.grid(True,Â linestyle="-")Â Â #Â AddÂ gridÂ lines
plt.xlim(0,Â 80000)Â Â #Â SetÂ theÂ x-axisÂ limits
plt.ylim(0,Â 600)Â Â #Â SetÂ theÂ y-axisÂ limits

##Â ShowÂ theÂ plot
plt.show()

#####Â TheÂ relationshipÂ ofÂ LoanAmountÂ andÂ ApplicantIncomeÂ #####
print('TheÂ relationshipÂ ofÂ LoanAmountÂ andÂ ApplicantIncome.\n')
sns.relplot(xÂ =Â 'LoanAmount',Â yÂ =Â 'ApplicantIncome',Â dataÂ =Â train)
plt.show()

print('*'*120,'\n')

#####Â TheÂ relationshipÂ ofÂ LoanAmountÂ andÂ ApplicantIncomeÂ withÂ GenderÂ #####
print('TheÂ relationshipÂ ofÂ LoanAmountÂ andÂ ApplicantIncomeÂ withÂ Gender\n')
sns.relplot(xÂ =Â 'LoanAmount',Â yÂ =Â 'ApplicantIncome',Â hueÂ =Â 'Gender',Â dataÂ =Â train)
plt.show()

print('*'*120,'\n')

#####Â TheÂ relationshipÂ ofÂ LoanAmountÂ andÂ ApplicantIncomeÂ withÂ Gender,Â andÂ self_employedÂ #####
print('TheÂ relationshipÂ ofÂ LoanAmountÂ andÂ ApplicantIncomeÂ withÂ Gender,Â andÂ SelfÂ Employed\n')
sns.relplot(xÂ =Â 'LoanAmount',Â yÂ =Â 'ApplicantIncome',Â hueÂ =Â 'Gender',Â colÂ =Â 'Self_Employed',Â dataÂ =Â train)
plt.show()

#Loan amount depend on the applicants income

#####Â showÂ theÂ maxÂ andÂ minÂ loanÂ #####
print('TheÂ maximumÂ homeÂ loanÂ amountÂ isÂ =Â {}Â '.format(train.loc[:,Â 'LoanAmount'].max(),'\n'))
print('TheÂ minimumÂ homeÂ loanÂ amountÂ isÂ =Â {}Â '.format(train.loc[:,Â 'LoanAmount'].min(),'\n'))
print('TheÂ averageÂ homeÂ loanÂ amountÂ isÂ =Â {}Â '.format(train.loc[:,Â 'LoanAmount'].mean(),'\n'))

#####Â ShowÂ theÂ distributionÂ ofÂ Loan_Amount_TermÂ #####
sns.distplot(train.loc[:,Â 'Loan_Amount_Term'])
plt.show()

print('\n')

sns.relplot(xÂ =Â 'Loan_Amount_Term',Â yÂ =Â 'LoanAmount',Â dataÂ =Â train)
plt.show()

#####Â FindÂ outÂ theÂ sum,Â meanÂ ofÂ IncomeÂ andÂ loanÂ ofÂ creaditÂ historyÂ #####
train.groupby(['Credit_History'])[['ApplicantIncome',Â 'CoapplicantIncome',Â 'LoanAmount','Loan_Amount_Term']].agg(['sum',Â 'mean'])

##Â GroupÂ theÂ dataÂ byÂ "Gender"Â andÂ "Loan_Status"Â andÂ calculateÂ theÂ count
loan_status_countÂ =Â pd.crosstab(train['Gender'],Â train['Loan_Status'])

##Â barÂ plot
loan_status_count.plot(kind="bar",Â stacked=False)

##Â labelsÂ andÂ title
plt.xlabel("Gender")
plt.ylabel("Count")
plt.title("LoanÂ StatusÂ CountÂ byÂ Gender",Â weight="bold")

##Â ShowÂ theÂ plot
plt.show()

train.groupby(["Education",Â "Loan_Status"])["Loan_Status"].count()

#Â CreatingÂ aÂ countÂ plotÂ forÂ "Loan_Status"Â withÂ theÂ hueÂ basedÂ onÂ "Education"
sns.countplot(data=train,Â x="Loan_Status",Â hue="Education")
plt.xlabel("LoanÂ Status")Â Â Â ##Â X-axisÂ label
plt.ylabel("Count")Â Â Â Â Â Â Â Â Â Â ##Â Y-axisÂ label
plt.title("LoanÂ StatusÂ byÂ Education",Â weight="bold")Â Â ##Â PlotÂ title
plt.legend(title="Education",Â loc="upperÂ right")Â ##Â legend
plt.show()

#Graduate's loan approvals are accepted more

print(train.groupby(["Loan_Status",Â "Credit_History"])["Loan_Status"].count())
print("*"*50)
sns.countplot(data=train,Â x="Loan_Status",Â hue="Credit_History")
plt.title("LoanÂ StatusÂ byÂ CreditÂ History.",Â weight="bold")
plt.show()

#Mostly, anyone with a credit history of 1 (Yes) will be accepted

sns.countplot(data=train,x='Loan_Status');

loan_status_marriedÂ =train.groupby(["Loan_Status",Â "Married"])["Married"].count().unstack()
loan_status_married

print("theÂ ratioÂ ofÂ Non-marriedÂ rejected".title(),
Â Â Â Â Â Â round((loan_status_married.iloc[0,0])Â /Â (loan_status_married.iloc[0,:].sum(axis=0)),2)*100)
print("theÂ ratioÂ ofÂ marriedÂ rejected".title(),
Â Â Â Â Â Â round((loan_status_married.iloc[1,0])Â /Â (loan_status_married.iloc[1,:].sum(axis=0)),2)*100)

train.groupby("Property_Area")["LoanAmount"].agg(["sum","mean","count"]).sort_values(by="mean",Â ascending=False)

#Â CreateÂ theÂ barÂ plot
sns.barplot(data=train,Â x="Property_Area",Â y="LoanAmount",Â errorbar=None,Â estimator="mean")

#Â CustomizeÂ theÂ plot
plt.title("MeanÂ LoanAmountÂ byÂ PropertyÂ Area",Â weight="bold")
plt.xlabel("PropertyÂ Area")
plt.ylabel("MeanÂ LoanAmount")
plt.xticks(rotation=45)Â Â #Â RotateÂ x-axisÂ labelsÂ forÂ betterÂ readability

#Â ShowÂ theÂ plot
plt.show()

#The average Loan Amount in the "Rural" area is higher than in the other areas.

#EducationÂ column
sns.countplot(train['Education'],palette='RdBu')

#SelfÂ employedÂ column
train['Self_Employed'].value_counts().plot(kindÂ =Â 'pie',autopctÂ =Â '%.2f',colors=['pink','violet'])

#PropertyÂ areaÂ column
train['Property_Area'].value_counts().plot(kindÂ =Â 'pie',autopctÂ =Â '%.2f',colors=['violet','pink','blue'])

#As indicated by the box plot there are many outliers that can deteriorate the model. Thus they need to be handled.

train.head()

#But first let us have a look at correlations between columns

train1=train.drop(['Loan_ID','Gender','Married','Dependents','Education','Self_Employed','Credit_History','Property_Area','Total_Income_bin','LoanAmount_bin'],axis='columns')
train1.corr()

train[train['ApplicantIncome']<train['CoapplicantIncome']]['Loan_Status'].value_counts()

##Â CreateÂ theÂ countplot
countplotÂ =Â sns.countplot(data=train,Â x="Self_Employed",Â hue="Loan_Status")Â Â #Â CustomizeÂ theÂ colorÂ palette

##Â AddÂ labelsÂ andÂ aÂ title
countplot.set(xlabel="SelfÂ Employed",Â ylabel="Count")
plt.title("CountÂ ofÂ LoanÂ StatusÂ byÂ SelfÂ Employment",Â weight="bold")

##Â setÂ legend
legendÂ =Â countplot.get_legend()
legend.set_title("LoanÂ Status")
forÂ t,Â lÂ inÂ zip(legend.texts,Â ["Approved",Â "NotÂ Approved"]):
Â Â Â Â t.set_text(l)

##Â axisÂ labelsÂ andÂ tickÂ labels
countplot.set_xticklabels(countplot.get_xticklabels(),Â rotation=45)Â Â #Â RotateÂ x-axisÂ labels

##Â showÂ plot
plt.show()

sns.pairplot(data=train,Â hue="Loan_Status")
plt.show()

#Â CalculateÂ theÂ correlationÂ betweenÂ 'LoanAmount'Â andÂ 'Loan_Amount_Term'
correlationÂ =Â train[["LoanAmount",Â "Loan_Amount_Term"]].corr().iloc[0,Â 1]

#Â PrintÂ theÂ correlationÂ value
print("CorrelationÂ betweenÂ LoanÂ AmountÂ andÂ LoanÂ AmountÂ TermÂ =Â {:.2f}".format(correlation))

#Â CreateÂ aÂ scatterplot
sns.scatterplot(data=train,Â x="LoanAmount",Â y="Loan_Amount_Term")

#Â SetÂ plotÂ labelsÂ andÂ title
plt.xlabel("LoanÂ Amount")
plt.ylabel("LoanÂ AmountÂ Term")
plt.title("ScatterplotÂ ofÂ LoanÂ AmountÂ vsÂ LoanÂ AmountÂ Term",Â weight="bold")

#Â ShowÂ theÂ plot
plt.show()

#Correlation between Numerical columns
Numerical_colsÂ =Â train.select_dtypes(include="number").columns.to_list()Â ##Â NumericalÂ FeaturesÂ inÂ theÂ data

##Â ComputeÂ theÂ correlationÂ matrix
correlation_matrixÂ =Â train.corr(numeric_only=True)

##Â CreateÂ theÂ heatmap
sns.heatmap(correlation_matrix,Â annot=True,Â cmap="RdBu_r",Â center=0)

plt.show()

#Correlation between Loan Amount and ApplicantIncome by Loan Status

##Â CreateÂ Scatterplot
scatterplotÂ =Â sns.scatterplot(data=train,Â x="ApplicantIncome",Â y="LoanAmount",Â hue="Loan_Status")

##Â titleÂ andÂ labels
plt.title("ApplicantÂ IncomeÂ vs.Â LoanÂ Amount",Â weight="bold")
plt.xlabel("ApplicantÂ Income")
plt.ylabel("LoanÂ Amount")

##Â legendÂ settings
scatterplot.legend(title="LoanÂ Status")
legend_labelsÂ =Â ["Approved",Â "NotÂ Approved"]Â Â #Â CustomÂ legendÂ labels
forÂ t,Â lÂ inÂ zip(scatterplot.get_legend().texts,Â legend_labels):
Â Â Â Â t.set_text(l)

##Â showÂ theÂ plot
plt.show()

#Feature Engineering

dataÂ =Â train.copy()

##Â TotalÂ Income
data["TotalIncome"]Â =Â data["ApplicantIncome"]Â +Â data["CoapplicantIncome"]

##Â CreateÂ KDEÂ plotÂ forÂ totalÂ income
sns.kdeplot(data=data,Â x="TotalIncome",Â fill=True)
plt.title("TotalÂ IncomeÂ DistributionÂ beforeÂ logÂ transform".title())
plt.show()

#EMI, which stands for Equated Monthly Installment, is a fixed payment made by a borrower to a lender at a specified date each month.

##Â EMI
data["EMI"]Â =Â data["LoanAmount"]Â /Â data["Loan_Amount_Term"]
data.columns.to_list()

train.replace({'Yes':1,'No':0},inplaceÂ =Â True)
train.head()

train.replace({'Graduate':1,'NotÂ Graduate':0},inplaceÂ =Â True)
train.head()

train.replace({'Urban':1,'Rural':0,'Semiurban':2},inplaceÂ =Â True)
train.head()

train.replace({'Y':1,'N':0},inplaceÂ =Â True)
train.head()

#ApplicantÂ incomeÂ column
train[train['ApplicantIncome']>50000]

train=Â train[train['ApplicantIncome']<50000]
train.shape

#CoapplicantÂ income
train[train['CoapplicantIncome']>=20000]

trainÂ =Â train[train['CoapplicantIncome']<20000]
train.shape

#LoanÂ amountÂ column
train[train['LoanAmount']>=600]

#train

train['Dependents'].unique()
array([nan])
train=train[train['LoanAmount']<600]
train.shape

train2Â =Â train[['ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Status']].copy()
train2.head()

#####Â CreateÂ aÂ featureÂ namedÂ annualÂ incomeÂ #####
train['annual_income']Â =Â train.loc[:,Â 'ApplicantIncome']*12

train.head()

#####Â What'sÂ theÂ ratioÂ ofÂ annualÂ incomeÂ basedÂ onÂ EducationÂ andÂ EmployementÂ #####
defÂ find_query(group):
Â Â returnÂ group['annual_income'].value_counts()

train.groupby([Â 'Education',Â 'Self_Employed']).apply(find_query).head()

#####Â FindÂ theÂ relationshipÂ ofÂ CoapplicantIncomeÂ Â Â Â andÂ ApplicantIncomeÂ #####
print('TheÂ relationshipÂ ofÂ CoapplicantIncomeÂ Â Â Â andÂ ApplicantIncomeÂ isÂ givenÂ below.\n')
sns.relplot(xÂ =Â 'CoapplicantIncome',Â yÂ =Â 'ApplicantIncome',Â dataÂ =Â train)
plt.show()

print('*'*120,'\n')

#####Â FindÂ theÂ relationshipÂ ofÂ CoapplicantIncomeÂ Â Â Â andÂ ApplicantIncomeÂ withÂ DependentsÂ #####
print('TheÂ relationshipÂ ofÂ CoapplicantIncomeÂ Â Â Â andÂ ApplicantIncomeÂ withÂ DependentsÂ isÂ givenÂ below.\n')
sns.relplot(xÂ =Â 'CoapplicantIncome',Â yÂ =Â 'ApplicantIncome',Â hueÂ =Â 'Dependents_',Â dataÂ =Â train)
plt.show()

sns.pairplot(train2,hueÂ =Â 'Loan_Status')

train1.corr()['Loan_Status']

#Now, moving on to handling text data

#As a machine learning model only understands numeric values, thus the text data or columns should be converted to some numeric equivalent. This is the reason all the yes and no were changed to 1, 0 and the column gender was also changed to numeric values.

train.info()

#Â SelectÂ numericalÂ columns
numerical_columnsÂ =Â train.select_dtypes(include=['number'])

#Â CalculateÂ theÂ correlationÂ matrix
matrixÂ =Â numerical_columns.corr()

#Â CreateÂ aÂ heatmap
f,Â axÂ =Â plt.subplots(figsize=(9,Â 6))
sns.heatmap(matrix,Â vmax=0.8,Â square=True,Â cmap="BuPu")

#Â ShowÂ theÂ plot
plt.show()

#We see that the most correlated variables are (ApplicantIncome â€“ LoanAmount) and (Credit_History â€“ Loan_Status). LoanAmount is also correlated with CoapplicantIncome.

train.head(1)

test.shape

train['Credit_History']Â =Â train['Credit_History'].astype(int)
train['Credit_History'].dtype
train.head()

train=train.drop(['Loan_ID','Gender','Married','Dependents','Education','Self_Employed','Credit_History','Property_Area','Total_Income_bin','LoanAmount_bin'],axis='columns')

#Data Splitting and Preprocessing

fromÂ sklearn.metricsÂ importÂ confusion_matrix
defÂ prediction_report(model,X_test,y_test,color):

Â Â Â Â #'''thisÂ functionÂ isÂ toÂ evaluateÂ theÂ model:
Â Â Â Â 1-->Â printÂ theÂ classificationÂ reportÂ Â Â Â Â 2-->Â displayÂ theÂ confusionÂ matrix'''

Â Â Â Â #testÂ report
Â Â Â Â y_pred_testÂ =Â model.predict(X_test)
Â Â Â Â print(classification_report(y_pred_test,y_test))
    
Â Â Â Â #testÂ confusionÂ matrix
Â Â Â Â plt.figure(figsize=(4,3))
Â Â Â Â sns.heatmap(confusion_matrix(y_test,y_pred_test),Â cmap=color,Â annot=True)
Â Â Â Â plt.xlabel('Predicted')
Â Â Â Â plt.ylabel('Actual')
Â Â Â Â plt.title('ConfusionÂ Matrix');
    
fromÂ sklearn.model_selectionÂ importÂ GridSearchCV
defÂ model_tunning(model,X_train,y_train,params):

Â Â Â Â #'''ThisÂ functionÂ recievesÂ aÂ modelÂ thenÂ tuneÂ itÂ usingÂ GridSearch
Â Â Â Â thenÂ printÂ theÂ bestÂ parametersÂ andÂ returnÂ theÂ bestÂ estimator'''

Â Â Â Â grid_searchÂ =Â GridSearchCV(model,Â param_grid=params,Â cvÂ =Â 5,Â scoring='f1')
Â Â Â Â grid_search.fit(X_train,y_train)
Â Â Â Â print(grid_search.best_params_)
Â Â Â Â print('MeanÂ cross-validatedÂ f1Â scoreÂ ofÂ theÂ bestÂ estimatorÂ is:Â ',grid_search.best_score_)
Â Â Â Â returnÂ grid_search.best_estimator_
    
fromÂ sklearn.model_selectionÂ importÂ train_test_split
featuresÂ =Â train.columns.drop(['Loan_Status','Loan_Amount_Term'])
targetÂ =Â 'Loan_Status'

XÂ =Â train[features]
yÂ =Â train[target]

num_featuresÂ =Â X.select_dtypes('number').columns
cat_featuresÂ =Â X.select_dtypes('object').columns

X_trainÂ ,Â X_testÂ ,Â y_trainÂ ,Â y_testÂ =Â train_test_split(XÂ ,Â yÂ ,Â random_state=42Â ,Â test_size=0.2Â ,Â stratifyÂ =Â y)

#Â importÂ libraries
fromÂ sklearn.pipelineÂ importÂ make_pipeline,Â Pipeline
fromÂ sklearn.imputeÂ importÂ SimpleImputer
fromÂ sklearn.preprocessingÂ importÂ OneHotEncoder,Â StandardScaler
fromÂ sklearn.composeÂ importÂ ColumnTransformer
fromÂ sklearn.linear_modelÂ importÂ LogisticRegression
fromÂ sklearn.metricsÂ importÂ accuracy_score
fromÂ sklearn.composeÂ importÂ make_column_transformer
fromÂ sklearn.imputeÂ importÂ KNNImputer

#numericalÂ featuresÂ pipeline
num_pipelineÂ =Â make_pipeline(
Â Â Â Â KNNImputer(),
Â Â Â Â StandardScaler(),
)
#categoricalÂ featuresÂ pipeline
cat_pipelineÂ =Â make_pipeline(
Â Â Â Â SimpleImputer(strategyÂ =Â 'most_frequent'),
Â Â Â Â OneHotEncoder(),
)
#combineÂ bothÂ pipelines
preprocessorÂ =Â make_column_transformer(
Â Â Â Â (num_pipeline,num_features),
Â Â Â Â (cat_pipeline,cat_features)
)

#Logistic Regression

log_regÂ =Â make_pipeline(
Â Â Â Â preprocessor,
Â Â Â Â LogisticRegression(random_state=42,Â solver='liblinear',Â max_iterÂ =Â 5000)
)

param_gridÂ =Â {
Â Â Â Â 'logisticregression__penalty':['l1','l2'],
Â Â Â Â 'logisticregression__C':[0.001,Â 0.01,Â 0.1,Â 1,Â 10,Â 100,Â 1000]
}

log_regÂ =Â model_tunning(log_reg,X_train,y_train,param_grid)

#trainingÂ report
fromÂ sklearnÂ importÂ metrics
fromÂ sklearn.metricsÂ importÂ classification_report
prediction_report(log_reg,X_train,y_train,'Blues')

#testÂ report
prediction_report(log_reg,X_test,y_test,'Blues')

#Support Vector Classifier (SVC)

fromÂ sklearn.svmÂ importÂ SVC
svmÂ =Â make_pipeline(
Â Â Â Â preprocessor,
Â Â Â Â SVC(kernelÂ =Â 'poly',random_state=42)
)

param_gridÂ =Â {
Â Â Â Â 'svc__C':[Â 0.01,Â 0.1,Â 1,Â 10,Â 100],
Â Â Â Â 'svc__degree':Â np.arange(2,5),
}

svmÂ =Â model_tunning(svm,X_train,y_train,param_grid)

#trainingÂ report
prediction_report(svm,X_train,y_train,'Greens')

prediction_report(svm,X_test,y_test,'Greens')

#KNN Classifier

fromÂ sklearn.neighborsÂ importÂ KNeighborsClassifier
knnÂ =Â make_pipeline(
Â Â Â Â preprocessor,
Â Â Â Â KNeighborsClassifier()
)

param_grid={
Â Â Â Â Â Â Â Â 'kneighborsclassifier__n_neighbors':range(1,21,2),
Â Â Â Â Â Â Â Â 'kneighborsclassifier__weights':['uniform','distance'],
Â Â Â Â Â Â Â Â 'kneighborsclassifier__metric':['euclidean','manhattan']
}

knnÂ =Â model_tunning(knn,X_train,y_train,param_grid)

#trainingÂ report
prediction_report(knn,X_train,y_train,'Reds')

#testingÂ report
prediction_report(knn,X_test,y_test,'Reds')

#Decision Tree Classifier

fromÂ sklearn.treeÂ importÂ DecisionTreeClassifier
dec_treeÂ =Â make_pipeline(
Â Â Â Â preprocessor,
Â Â Â Â DecisionTreeClassifier()
)
param_gridÂ =Â {
Â Â Â Â 'decisiontreeclassifier__max_depth':Â np.arange(2,Â 15),
Â Â Â Â 'decisiontreeclassifier__min_samples_split':Â np.arange(2,Â 7),
Â Â Â Â 'decisiontreeclassifier__min_samples_leaf':Â np.arange(1,Â 6),
}

dec_treeÂ =Â model_tunning(dec_tree,X_train,y_train,param_grid)

prediction_report(dec_tree,X_train,y_train,'Blues')

prediction_report(dec_tree,X_test,y_test,'Blues')

#Random Forest Classifier

fromÂ sklearn.ensembleÂ importÂ RandomForestClassifier
rfcÂ =Â make_pipeline(
Â Â Â Â preprocessor,
Â Â Â Â RandomForestClassifier(n_jobs=-1)
)
param_gridÂ =Â {
Â Â Â Â 'randomforestclassifier__max_depth':Â np.arange(2,Â 8),
Â Â Â Â 'randomforestclassifier__n_estimators':Â np.arange(10,Â 101,Â 10),
}
rfcÂ =Â model_tunning(rfc,X_train,y_train,param_grid)

prediction_report(rfc,X_train,y_train,'Blues')

prediction_report(rfc,X_test,y_test,'Blues')

#Conclusion The best performing model is random forest classifier with f1 score of 81% on training set and 83 % score on test set.

fromÂ sklearn.model_selectionÂ importÂ train_test_splitX_train,X_test,y_train,y_testÂ =Â train_test_split(X,y,test_size=0.2,random_state=10)

fromÂ sklearn.linear_modelÂ importÂ LogisticRegression
fromÂ sklearn.neighborsÂ importÂ KNeighborsClassifier
fromÂ sklearn.treeÂ importÂ DecisionTreeClassifier,Â export_graphviz
fromÂ sklearn.ensembleÂ importÂ RandomForestClassifier
fromÂ sklearn.svmÂ importÂ SVC
fromÂ sklearn.model_selectionÂ importÂ GridSearchCV
fromÂ sklearn.model_selectionÂ importÂ ShuffleSplit
fromÂ sklearn.model_selectionÂ importÂ cross_val_score
fromÂ sklearn.metricsÂ importÂ accuracy_score,Â f1_score,Â confusion_matrix
importÂ graphviz

#Now that we know random forest classifier works better on the data, lets train the model.

X_train.columns

y_train.unique()

defÂ select_best_model(x,y):
Â Â Â Â algosÂ =Â {
Â Â Â Â Â Â Â Â 'KNN'Â :Â {
Â Â Â Â Â Â Â Â Â Â Â Â 'model'Â :Â KNeighborsClassifier(),
Â Â Â Â Â Â Â Â Â Â Â Â 'params'Â :Â {
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 'n_neighbors'Â :Â [13,15],
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 'weights'Â :Â ['uniform',Â 'distance']
Â Â Â Â Â Â Â Â Â Â Â Â }
Â Â Â Â Â Â Â Â },
Â Â Â Â Â Â Â Â 'Random_forest_classifier'Â :Â {
Â Â Â Â Â Â Â Â Â Â Â Â 'model'Â :Â RandomForestClassifier(),
Â Â Â Â Â Â Â Â Â Â Â Â 'params'Â :Â {
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 'n_estimators'Â :Â [150,250],
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 'bootstrap'Â :Â [True,False]
Â Â Â Â Â Â Â Â Â Â Â Â }
Â Â Â Â Â Â Â Â }
Â Â Â Â }
Â Â Â Â scores=[]
Â Â Â Â cvÂ =Â ShuffleSplit(n_splits=5,test_size=0.2,random_state=0)
Â Â Â Â forÂ algos_name,Â configÂ inÂ algos.items():
Â Â Â Â Â Â Â Â gsÂ =Â GridSearchCV(config['model'],config['params'],cv=cv)
Â Â Â Â Â Â Â Â gs.fit(x,y)
Â Â Â Â Â Â Â Â scores.append({
Â Â Â Â Â Â Â Â Â Â Â Â 'model'Â :Â algos_name,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â "best_params"Â :gs.best_params_,
Â Â Â Â Â Â Â Â Â Â Â Â "best_score"Â :Â gs.best_score_
Â Â Â Â Â Â Â Â })
Â Â Â Â returnÂ pd.DataFrame(scores,Â columns=['model',Â 'best_params',Â 'best_score'])
select_best_model(X,y)

#Now that we know random forest classifier works better on the data, lets train the model.

rf_clfÂ =Â RandomForestClassifier(n_estimators=250)
rf_clf.fit(X_train,y_train)

y_predÂ =Â rf_clf.predict(X_test)
y_pred

fromÂ sklearn.metricsÂ importÂ confusion_matrix
cm=confusion_matrix(y_test,y_pred)
sns.heatmap(cm,annot=True)

fromÂ sklearn.metricsÂ importÂ mean_squared_error
mean_squared_error(y_test,y_pred)

rf_clf.score(X_test,y_test)

With this the Notebook comes to its conclusion, following are the conclusions:

#The data had many null values. Thus, one by one all the null values were filled. There were some outliers in the data which was also handled. The model performed the best on random forest among the two tested. Giving the model an error of approx. 24% with some wrong predictions but all in all the model performed fine. Now, the accuracy can be increased or the error can be decreased by testing the data on some other algorithm and by doing some more hyperparameter tuning. But for now this is it.

Using K-Means Clustering in this test dataframe

#####Â PreprocessingÂ theÂ testÂ dataÂ frameÂ #####
fromÂ sklearn.preprocessingÂ importÂ MinMaxScaler
MinMaxScaler_Â =Â StandardScaler()
test_scaledÂ =Â pd.DataFrame(MinMaxScaler_.fit_transform(X_test),Â columnsÂ =Â X_test.columns)
test_scaled.head()

fromÂ sklearn.clusterÂ importÂ KMeans
fromÂ sklearn.metricsÂ importÂ silhouette_score
wcss_Â =Â []
forÂ n_clusterÂ inÂ range(2,Â 5):
Â Â KMeans_Â =Â KMeans(n_clustersÂ =Â n_cluster,Â max_iterÂ =Â 500)
Â Â KMeans_.fit_predict(X_test)
Â Â wcss_.append(KMeans_.inertia_)
Â Â print(f'TheÂ scoreÂ isÂ =Â {silhouette_score(X_test.values,Â KMeans_.labels_)}Â forÂ n_clusterÂ =Â {n_cluster}')

#####Â TakeÂ n_clusterÂ =Â 2Â inÂ KMeansÂ #####
KMeans_Â =Â KMeans(n_clustersÂ =Â 2,Â max_iterÂ =Â 500)
target_Â =Â KMeans_.fit_predict(X_test)
print(target_)

df1_Â =Â pd.concat([X_test,Â pd.DataFrame(y,Â columnsÂ =Â ['Loan_Status'])],Â axisÂ =Â 1,Â ignore_indexÂ =Â True)
df1_.head()

pd.DataFrame(MinMaxScaler_.fit_transform(pd.get_dummies(dataÂ =Â X_train,Â drop_firstÂ =Â True)))

new_dfÂ =Â pd.concat([X_train,Â X_test],Â axisÂ =Â 0,Â ignore_indexÂ =Â True)
new_df.head()

fromÂ sklearn.linear_modelÂ importÂ LogisticRegression
fromÂ sklearn.model_selectionÂ importÂ GridSearchCV
clfÂ =Â GridSearchCV(LogisticRegression(),{'C':[0.001,0.01,0.1,1,10]},Â cv=10).fit(X_train,Â y_train)
print("BestÂ ParamsÂ "Â +Â str(clf.best_params_)Â +Â "Â BestÂ ScoreÂ "Â +Â str(clf.best_score_))

#buildingÂ modelÂ 1Â withÂ randomÂ forestÂ classifierÂ andÂ predictÂ labels
model1=RandomForestClassifier(n_estimators=600,max_depth=10)
model1.fit(X_train,y_train)
tr_pred=model1.predict(X_train)
print(classification_report(y_train,tr_pred))

#predictionsÂ withÂ X_valÂ dataÂ set
ts_pred=model1.predict(X_test)
print(classification_report(y_test,ts_pred))

#modelÂ 2Â withÂ HistGradientBoostingClassifier
fromÂ sklearn.ensembleÂ importÂ HistGradientBoostingClassifier
model2=HistGradientBoostingClassifier(max_iter=800,learning_rate=0.01)
model2.fit(X_train,y_train)
ts_pred=model2.predict(X_test)
print(classification_report(y_test,ts_pred))

#modelÂ 3Â withÂ logisticRegression
model3=LogisticRegression(max_iter=2000)
model3.fit(X_train,y_train)
ts_pred=model3.predict(X_test)
print(classification_report(y_test,ts_pred))

fromÂ sklearn.linear_modelÂ importÂ LogisticRegression
fromÂ sklearn.model_selectionÂ importÂ GridSearchCV
clfÂ =Â GridSearchCV(LogisticRegression(),{'C':[0.001,0.01,0.1,1,10]},Â cv=10).fit(X_train,Â y_train)
print("BestÂ ParamsÂ "Â +Â str(clf.best_params_)Â +Â "Â BestÂ ScoreÂ "Â +Â str(clf.best_score_))

fromÂ sklearn.model_selectionÂ importÂ cross_val_predict
ypÂ =Â cross_val_predict(clf,Â X_train,Â y_train,Â cv=10)
fromÂ sklearn.metricsÂ importÂ classification_report
print(classification_report(y_train,Â yp))

fromÂ sklearn.metricsÂ importÂ ConfusionMatrixDisplay
importÂ matplotlib.pyplotÂ asÂ plt
ConfusionMatrixDisplay.from_predictions(y_train,Â yp,Â cmap="Blues")
plt.show()

yptÂ =Â clf.predict(X_test)
print(classification_report(y_test,Â ypt))

fromÂ sklearn.metricsÂ importÂ ConfusionMatrixDisplay
importÂ matplotlib.pyplotÂ asÂ plt
ConfusionMatrixDisplay.from_predictions(y_test,Â ypt,Â cmap="Blues")
plt.show()

Feature Selection Wrapper (Forward)

fromÂ sklearn.model_selectionÂ importÂ GridSearchCV
fromÂ sklearn.svmÂ importÂ SVC
fromÂ sklearn.feature_selectionÂ importÂ SequentialFeatureSelector
swÂ =Â SequentialFeatureSelector(SVC(),n_features_to_select=5,direction='forward',Â cv=5).fit(X_train,Â y_train)
X_train_norm_swÂ =Â sw.transform(X_train)
X_test_norm_swÂ =Â sw.transform(X_test)
clfÂ =Â GridSearchCV(SVC(),{'C':[1,2,4,8,16,32]},Â cv=10).fit(X_train_norm_sw,Â y_train)

ypÂ =Â cross_val_predict(clf.best_estimator_,Â X_train_norm_sw,Â y_train,Â cv=10)
print(classification_report(y_train,Â yp))
ConfusionMatrixDisplay.from_predictions(y_train,Â yp,Â cmap="Blues")
plt.show()

yptÂ =Â clf.predict(X_test_norm_sw)
print(classification_report(y_test,Â ypt))
ConfusionMatrixDisplay.from_predictions(y_test,Â ypt,Â cmap="Blues")
plt.show()
